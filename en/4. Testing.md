# 4. Testing

limit tests timeout

Don't embed test data in tests, assign it first to a variable, or if possible, use fixtures

git hooks to run tests on `pre-commit` and `pre-push`
Git hooks to run linting and tests and generate docs before each commit and push. If tests are heavy, only linting and docs for commits is fine, but tests are mandatory before push

Check actual returned value on tests, not only status
- [ ] no git hooks to exec style linting and tests before push new code

- [ ] Set `jasmine-node` as `devDependency`, not `dependency`
- [ ] Replace `jasmine-node` test runner and `should` assertions suite for
      integrated, compatible and better mainteined `jest` module
- [ ] Enable code coverage (`jest` has it integrated by free)
- [ ] Tests has commented test statements
- [ ] Use descriptive and test agnostic tests and suites names
- [ ] Don't have hardcoded URL queries, generate them with the `URL` built-in
- [ ] Validate returned data, not just only that's not empty
- [ ] Use `Promise` based tests instead of `done` callback
- [ ] Duplicated test statement

> **TL;DR**
>
> - Write tests. They can be run by hand, but always define tests for your code.

Tests allow you to validate the code does what's expected, and without them we
can consider the code behaviour random and by definition "broken". They are the
most useful, and at the same time the most infravalued, code you can write. This
is because writting good tests consume time, and it's code that will not be
running later in production, but they would help you to find bugs and errors
earlier and reduce develop time in the long term. Consider them part of the
code you write, and do both your code and its tests at the same time.

Ideally tests should be automated and replicable, but sometimes define the tests
environment can be complicated, for example when they involve actions in the
real world, like print a document. In that contexts, it's easier to consider
tests in a more wide sense like a list of validations that needs to pass each
time the tests are check. This can be implemented as a simple checklist or
bullet points and be given to another person (like a QA department) to validate
them by hand, and when oportunity arise automate them to make the testing stage
easier and faster.

Regarding automated test frameworks, there are several ones for Node.js, but the
*de-facto* current standard is [jest](https://jestjs.io/). Developed by
Facebook, it's a one-stop framework that holds capabilities of both testing
framework, tests runner, assertions and code coverage, and also it support
testing async functionality when the test return a Promise. It's very focused
to be used with [React](https://reactjs.org/), so the default environment it
uses is browser-like based on [JsDOM](https://github.com/jsdom/jsdom), if you
want to use a real Node.js environment (for example, to use Javascript features
available in latests versions of Node.js), use the
[testEnvironment](https://jestjs.io/docs/en/configuration#testenvironment-string)
option set to `node`.

## Unit tests, and acceptance tests

> **TL;DR**
>
> - Do the tests only against the public API (*acceptance tests*).

Probably you've read about *unit tests* in the past, but there's a bit of
confussion here, and probably you would not need them. Unit testing regards to
fine-grain testing of all the component methods and behaviour, that's overkill
and froze the code details, and sometimes is useless.

What's more people really need (and some times misnames as *unit tests*) are
*acceptance tests*: tests done against the public API of your component,
whatever is it. If you are writting a class, its public API are its public
methods; if you are writting a REST API, its public API are its web endpoints.
You don't need to worry about the internal details how they are done, just only
that when you call them, you receive the expected result. This make tests a bit
more dificult to write if the use cases/stories are complex, but provides more
fidelity about how the components are going to be used (at the same time the
code coverage is already being improved), isolate the tests from the
mplementation (tests can be reused if libraries are changed, and in some cases,
also when changing programming languages too), helps to improve actual APIs to
make them more usable, and helps to identify whats parts of the code are not
accesible from the public API (garbage code) and could be removed instead of
writting tests for them.

If you need to do unit tests for specific behaviour of a private component,
move that component to a new module and require it directly in your tests, or
also consider to move it to an external package and use it as a dependency and
implementation detail. This way, that private components becames the public API
of the new module/package, so you can write acceptance tests against it.

Jest tests use by default all the Javascript files located in the `__tests__`
folder, and also any file with a `.test.js` or `.spec.js` extension in your
source code next to the module they are testing. The standard way is to use the
`__tests__` folder for all the tests, so later they can be easily identified and
removed when publishing the package, but it's also ok to use the files with
`.test.js` extension. Sometimes both are being used to differenciate between
acceptation tests (using the `__tests__` folder) and unit tests (using files
with `.test.js` next to the module they are testing), but it's not so common.

### Specs, Test Driven Development and Behaviour Driven Development with Jest

> **TL;DR**
>
> - Write a *spec* of the project behaviour and interfaces before start coding.

A spec clearly define without ambiguity what your code needs to do, and how it
will interconnect with other components or interact with the user. If it's
correctly written in detail, it's enough to implement the code without further
clarifications. There are a lot of public specs and standards you can get as
reference to write yours like [ietf RFCs](https://www.ietf.org/standards/rfcs/)
or [w3c standards](https://www.w3.org/TR/).

Jest APIs make it very intuitive to create tests and to follow Test or
Behaviour Driven Development patterns (*TDD* and *BDD*, respectively) based on
specs. First of all, it's possible to define the tests with just only their
description and no test code at all, and they will be shown as pending to be
implemented, but you can also set it explicitly by using Jest
[`test.todo()`](https://jestjs.io/docs/en/api#testtodoname) function. On the
other hand, it's also posible to define nested tests groups (or *suites*), so
you can group related tests or also define them in a scoped hierachy, for
example defining an upper group for all the tests of a class, later smaller
groups for the tests of each class method, and finally having the tests for each
one of the use cases. Additionally, Jest creates itself a tests group for each
test file using the test file name as description, so there's no need to create
an explicit tests group by hand to group all of them.

Knowing that, converting the spec file to some tests suites is trivial, you just
only need to create tests groups as needed by the spec descriptions, and add
tests pending to be filled to test their behaviour (that's the reason why Jest
can be able to identify files with `.spec.js` extension as tests files, too).
It's important to name the tests description to what they are testing, not to
what you are expecting to receive: this expectations are called *assertions*,
and you can have several assertions on a single test. For example, you can have
a test for a request to a HTTP endpoint with some specific parameters, and this
test having several assertions about the result status code, its returned value,
the response headers, its content-type...

### Code coverage

> **TL;DR**
>
> - Check tests code coverage. Under 80% is garbage, try to achieve 100%.

Code coverage is an important part of tests, since it helps you to identify what
lines of your code are untested, and in some cases what decision branches in
your code logic are tested and what ones not. `Jest` provide support for that,
but it's disabled by default. To enable it, you can use the `--coverage` flag,
and will show some code coverage statistics after passing the tests. By default,
`jest` only collect coverage info from the files that has tests for, if you want
to be sure all your project files are tested, use the
[collectCoverageFrom](https://jestjs.io/docs/en/configuration#collectcoveragefrom-array)
config option.

### Isolate tests environment

> **TL;DR**
>
> - Isolate your tests, don't depend on external infraestructure to run them.
> - If you need infraestructure for your tests, they are *integration tests*.

One common pitfall when writting unitary and acceptance tests is that they are
testing our own code, but still accessing third party infraestructure. This can
lead to problems from tests not passing because the network is not available
(i.e. developing in a plane) to adding garbage data in real databases (in
development stage, or in production), incurring in costs each time tests are
being executing (i.e. sending confirm SMSs for a login system), or needing some
external hardware like a ticketing printer or a barcode scanner.

To deal with this, there are options like dependencies injection usual in other
languages, but over complicated in Javascript. In that case, it's better to use
mocks, both by allowing to pass an optional argument to the class constructors
about what object instance you want to use (but allowing it to create a regular
one by default) and setting it to the mock object during the tests, or use an
already available mock or dumb object for the service or device you want to use.

To mock HTTP requests to external servers when testing a client, the best tool
is [nock](https://github.com/nock/nock). It allow to capture real requests to
HTTP servers like third party APIs and later in your tests intercept them and
return instead the recorded data, or any one you provide it. On the other hand,
if what you are testing is a server, it's counterpart is
[supertest](https://github.com/visionmedia/supertest), that allow to customize
HTTP request and do expectations about its output. It can work with Promises, so
it integrates really well with `jest`.

Databases are a more complicate subject, because usually they are a standalone
server, so they should be considered *integration tests*, but are tightly
coupled with the application code, also when databases accesses are acotated to
a single file or group of files. When they offer an HTTP interface it's possible
to `nock` as explained before. In other cases, the traditional solution has been
to require to spin up a local database server before running the tests and use
them. This is a somewhat good idea, but have the problem of filling the database
with garbage and waste space or get bad results due to old data. In the case of
MongoDB database, it's better to use some tool like
[MongoBox](https://github.com/piranna/mongobox.js), that each time starts an
in-memory empty instance of MongoDB in a random TCP port, so there's no conflict
with other previous servers (and also can start several instances at the same
time) and you can fill it with the fixtures needed for each one of the tests,
having a deterministic and controlled tests environment.

For other components like WebSocket servers or when your tests require some
external hardware, there are some modules that can be useful available in
[npm registry](https://www.npmjs.com), it's just a matter of search for the one
that bests fits. If not, you can try to write some dumb objects that mimick
their behaviour or the one of their interfaces (maybe writing the data with some
delays, or logging it to the console or to a file for later inspection), and
set your code to use them.

## Integration tests

> **TL;DR**
>
> - Write *integration tests* using real infraestructure in an umbrella project.

If you need to connect to other real servers or systems, or integrate your code
with another one (like writting a plugin or module for another system) to test
your code in a real environment without mocks, then they can not be considered
unitary or acceptance test anymore, since they are now testing the integration
with other systems. These *integration tests* are usually are slower to boot-up
and more time consuming due to being more in detail, and could have some
associated costs too by using these real systems, so they should be run only in
a *Continuous Integration* server, for example each time your code is uploaded
using `git push`, or when debugging integration problems and bugs. Take in
account that if unit and acceptance tests and their mocks are given attention to
be properly developed and configured, it could be possible just by changing them
to use real systems to make them work as integration tests too, so it compensate
the extra effort.

Integration tests infraestructure is sometimes implemented by installing the
needed dependencies globally in the system or by using external servers, but if
they are not cleaned, they accumulate garbage or you can use old data that get
to wrong results, and there could be interference with other processes or users
passing the tests at the same time too, leading to conflicts. To fix that, not
only clean the tests environment before and after each test, but if possible
also isolate them. For this, using some tools like MongoBox (explained before)
or other similar ones where blank-state single-use (and if possible memory-only)
instances are used for each one of the tests is the best option, but if they
don't provide this possibility or it's difficult to configure, an alternative
can be to use some containerization tool like [Docker](https://www.docker.com/)
or [Vagga](https://github.com/tailhook/vagga) or a virtual machine and a
read-only snapshot of the system on that initial blank state, and run each one
of the tests on top of it discarding the changes after each one and starting
again for each new one.

Automated tests for user interaction are the most complex ones, but still
feasable. For command line tools, it's possible to wrap their stdio streams by
using packages like [suppose](https://www.npmjs.com/package/suppose), so it's
possible to "write" data and "read" its output the same way would be done by an
user on a terminal (in fact, it's launching and managing a pseudo-terminal under
the hood just only for this command). For web development, it's possible to use
tools like [PhantomJS](https://phantomjs.org/) or
[Selenium](https://www.seleniumhq.org/) to control a real web browser, interact
with it the same way a user would do, and fetch the returned data to inspect if
the results are the expected.

Finally, if you need to test real hardware, sometimes it's possible to plug some
dumb interfaces that can be controlled programatically or output can be sampled
and digitalized, having an interface similar to the previously named *suppose*
but in hardware. In some extreme cases it's possible to use some actuators and
videocameras to monitor and automate the process, but these options can be
forbiddenly expensive. All the major issues and pitfalls should have already
been detected in the previous testing stages and would remain just only very
specific corner cases, so in that extremes cases, it can be easier and cheaper
just to have list of check points and left them to an expert from QA department
to validate them by hand.

## System tests

> **TL;DR**
>
> - Implement *system tests* to be sure everything works after deployment.

Deployment of the code can fail too, and *system tests* allow to detect that
issues early. They check that once your code is put in production everything is
working as expected in the real environment: servers are accesible, mobile apps
can be downloaded an installed, users can login, in-app purchases are working...
These tests should be done inmediatly after the deployment is done, ideally by
the continuous integration server itself in case you are using it to do
continuous deployment or rolling release (as you should do).

System tests must not be confused with monitoring systems and health checks.
This last ones are focused on checking that the systems doesn't have failures or
downtimes in production, and their checking and maintenance are responsability
of support department.
